torch>=2.0.0
transformers>=4.40.0
datasets>=2.14.0
accelerate>=0.27.0
tokenizers>=0.15.0
sentencepiece>=0.1.99
protobuf>=3.20.0
tensorboard>=2.14.0
numpy>=1.24.0
tqdm>=4.65.0
pydantic>=2.0.0

# Optional dependencies
# wandb>=0.15.0  # For W&B logging
# flash-attn>=2.5.0  # For flash attention (requires CUDA)
